{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80cc04b4",
   "metadata": {},
   "source": [
    " The network architecture\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec4bf747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b4494b",
   "metadata": {},
   "source": [
    "- The core building block of neural networks is the layer, a data-processing module that\n",
    "you can think of as a filter for data. Some data goes in, and it comes out in a more useful form. Specifically, layers extract representations out of the data fed into them—hopefully, representations that are more meaningful for the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31f8fbd",
   "metadata": {},
   "source": [
    "The engine of neural networks: Gradient-based optimization\n",
    "-    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "52659b42",
   "metadata": {},
   "source": [
    "output = relu(dot(input, W) + b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be803dba",
   "metadata": {},
   "source": [
    "-  We have three tensor operations here:\n",
    "    -  A dot product (dot) between the input tensor and a tensor named W\n",
    "    -  An addition (+) between the resulting matrix and a vector b\n",
    "    -  A relu operation: relu(x) is max(x, 0); “relu” stands for “rectified linear unit”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bfc588",
   "metadata": {},
   "source": [
    "- In this expression, W and b are tensors that are attributes of the layer. They’re called the weights or trainable parameters\n",
    "of the layer (the kernel and bias attributes, respectively). These weights contain the information learned by the model from \n",
    "exposure to training data.\n",
    "\n",
    "-  Initially, these weight matrices are filled with small random values (a step called\n",
    "   random initialization). Of course, there’s no reason to expect that relu(dot(input, W)+ b), \n",
    "   when W and b are random, will yield any useful representations. The resulting\n",
    "    representations are meaningless—but they’re a starting point. What comes next is to\n",
    "    gradually adjust these weights, based on a feedback signal. This gradual adjustment,\n",
    "    also called training, is the learning that machine learning is all about."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4eb6ce19",
   "metadata": {},
   "source": [
    "\n",
    " An input vector, x (a sample in a dataset)\n",
    " A matrix, W (the weights of a model)\n",
    " A target, y_true (what the model should learn to associate to x)\n",
    " A loss function, loss (meant to measure the gap between the model’s current\n",
    "  predictions and y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964a4818",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
